{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55376f60",
   "metadata": {},
   "source": [
    "# n-gram 언어 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62718e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4800d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(오늘은|날씨가) = 0.400\n",
      "P(날씨가|좋다) = 0.500\n",
      "P(좋다|.) = 1.000\n",
      "P(.|오늘은) = 0.800\n",
      "P(오늘은|기분이) = 0.200\n",
      "P(기분이|좋다) = 1.000\n",
      "P(오늘은|일이) = 0.200\n",
      "P(일이|많다) = 1.000\n",
      "P(많다|.) = 1.000\n",
      "P(오늘은|사람이) = 0.200\n",
      "P(사람이|많다) = 1.000\n",
      "P(날씨가|맑다) = 0.500\n",
      "P(맑다|.) = 1.000\n"
     ]
    }
   ],
   "source": [
    "text = \"오늘은 날씨가 좋다. 오늘은 기분이 좋다. 오늘은 일이 많다. 오늘은 사람이 많다. 오늘은 날씨가 맑다.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# 1-gram, 2-gram 생성 -> 빈도수 계산\n",
    "unigrams = tokens\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "unigram_freq = Counter(unigrams)\n",
    "bigrams_freq = Counter(bigrams)\n",
    "# print(bigrams_freq)\n",
    "\n",
    "# 조건부 확률 계산\n",
    "for (w1, w2), freq in bigrams_freq.items():\n",
    "    prob = freq / unigram_freq[w1]\n",
    "    print(f'P({w1}|{w2}) = {prob:.3f}')     # 확률은 w1 뒤에 w2가 올 확률  ('오늘은' 4번 등장, 그 중에 뒤에 '날씨가' 등장한 것은 1번 => 0.25 확률)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a4a69",
   "metadata": {},
   "source": [
    "### Perplexity 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acda1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def compute_bigram_perplexity(test_text, unigram_freq, bigrams_freq):\n",
    "    test_tokens = nltk.word_tokenize(test_text)\n",
    "    test_bigrams = list(ngrams(test_tokens, 2))\n",
    "\n",
    "    log_prob_sum = 0\n",
    "    N = len(test_bigrams)\n",
    "\n",
    "    for bigram in test_bigrams:\n",
    "        w1, w2 = bigram\n",
    "        prob = bigrams_freq.get(bigram, 0) / unigram_freq.get(w1, 1)\n",
    "        if prob == 0:\n",
    "            prob = 1e-10\n",
    "        log_prob_sum += math.log2(prob)\n",
    "\n",
    "    cross_entropy = -log_prob_sum / N\n",
    "    perplexity = math.pow(2, cross_entropy)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a6f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = \"자연어 처리는 재미있다. 자연어 처리는 어렵지만 도전하고 싶다. 오늘은 날씨가 좋다.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "train_tokens =  nltk.word_tokenize(train_text)\n",
    "\n",
    "# 유니그램\n",
    "unigrams = train_tokens\n",
    "\n",
    "# 바이그램(리스트로 변환)\n",
    "bigrams = list(ngrams(train_tokens, 2))\n",
    "\n",
    "# 카운터로 빈도수 카운트(유니그램)\n",
    "unigram_freq = Counter(unigrams)\n",
    "\n",
    "# 카운터로 빈도수 카운트(바이그램)\n",
    "bigram_freq = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ba474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 자연어 처리는 재미있다. | perplexity: 1.2599\n",
      "문장: 자연어 처리는 어렵지만 도전하고 싶다. | perplexity: 1.1487\n",
      "문장: 오늘은 날씨가 좋다 | perplexity: 1.0000\n",
      "문장: 기계 번역은 어렵다. | perplexity: 10000000000.0000\n",
      "문장: 자연어 처리에 도전하고 싶다. | perplexity: 100000.0000\n",
      "문장: 오늘 날씨가 흐리다 | perplexity: 10000000000.0000\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"자연어 처리는 재미있다.\",\n",
    "    \"자연어 처리는 어렵지만 도전하고 싶다.\",\n",
    "    \"오늘은 날씨가 좋다\",\n",
    "    \"기계 번역은 어렵다.\",\n",
    "    \"자연어 처리에 도전하고 싶다.\",\n",
    "    \"오늘 날씨가 흐리다\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    pp = compute_bigram_perplexity(sentence, unigram_freq, bigram_freq)\n",
    "    print(f'문장: {sentence} | perplexity: {pp:.4f}')\n",
    "\n",
    "# 앞과 뒤 맥락에 대한 것을 엮어서 이어지는 단어들의 등장확률을 조건부확률로 계산한 것을 기반으로 한다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
