{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3a7aee",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efb972",
   "metadata": {},
   "source": [
    "- **Word Embedding**은 단어를 고정된 차원의 벡터로 변환하는 기술로, 단어 간의 의미적 유사성을 반영하도록 학습된 벡터를 말한다.\n",
    "- 이 기술은 자연어 처리에서 문장을 처리하고 이해하는 데 활용된다.\n",
    "- 숫자로 표현된 단어 목록을 통해 감정을 추출하는 것도 가능하다.\n",
    "- 연관성 있는 단어들을 군집화하여 다차원 공간에 벡터로 나타낼 수 있으며, 이는 단어나 문장을 벡터 공간에 매핑하는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63c193",
   "metadata": {},
   "source": [
    "**Embedding Matrix 예시**\n",
    "\n",
    "*아래 표의 벡터 값들은 모두 기계 학습을 통해 학습된 결과이다.*  \n",
    "\n",
    "| Dimension | Man (5391) | Woman (9853) | King (4914) | Queen (7157) | Apple (456) | Orange (6257) |\n",
    "|-----------|------------|--------------|-------------|--------------|-------------|---------------|\n",
    "| 성별      | -1         | 1            | -0.95       | 0.97         | 0.00        | 0.01          |\n",
    "| 귀족      | 0.01       | 0.02         | 0.93        | 0.95         | -0.01       | 0.00          |\n",
    "| 나이      | 0.03       | 0.02         | 0.7         | 0.69         | 0.03        | -0.02         |\n",
    "| 음식      | 0.04       | 0.01         | 0.02        | 0.01         | 0.95        | 0.97          |\n",
    "\n",
    "<br>\n",
    "\n",
    "*아래는 전치된 표이다.*\n",
    "\n",
    "| Word          | 성별   | 귀족   | 나이   | 음식   |\n",
    "|---------------|--------|--------|--------|--------|\n",
    "| Man (5391)    | -1.00  | 0.01   | 0.03   | 0.04   |\n",
    "| Woman (9853)  | 1.00   | 0.02   | 0.02   | 0.01   |\n",
    "| King (4914)   | -0.95  | 0.93   | 0.70   | 0.02   |\n",
    "| Queen (7157)  | 0.97   | 0.95   | 0.69   | 0.01   |\n",
    "| Apple (456)   | 0.00   | -0.01  | 0.03   | 0.95   |\n",
    "| Orange (6257) | 0.01   | 0.00   | -0.02  | 0.97   |\n",
    "\n",
    "- **의미적 유사성 반영**  \n",
    "  - 단어를 고정된 크기의 실수 벡터로 표현하며, 비슷한 의미를 가진 단어는 벡터 공간에서 가깝게 위치한다.  \n",
    "  - 예를 들어, \"king\"과 \"queen\"은 비슷한 맥락에서 자주 사용되므로 벡터 공간에서 가까운 위치에 배치된다.  \n",
    "\n",
    "- **밀집 벡터(Dense Vector)**  \n",
    "  - BoW, DTM, TF-IDF와 달리 Word Embedding은 저차원 밀집 벡터로 변환되며, 차원이 낮으면서도 의미적으로 풍부한 정보를 담는다.  \n",
    "  - 벡터 차원은 보통 100 또는 300 정도로 제한된다.  \n",
    "\n",
    "- **문맥 정보 반영**  \n",
    "  - Word Embedding은 단어 주변의 단어들을 학습해 단어의 의미를 추론한다.  \n",
    "  - 예를 들어, \"bank\"라는 단어가 \"river\"와 함께 나오면 \"강둑\"을, \"money\"와 함께 나오면 \"은행\"을 의미한다고 학습한다.  \n",
    "\n",
    "- **학습 기반 벡터**  \n",
    "  - Word Embedding은 대규모 텍스트 데이터에서 단어 간 연관성을 학습해 벡터를 생성한다.  \n",
    "  - 반면, BoW나 TF-IDF는 단순한 규칙 기반 벡터화 방법이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8583cf",
   "metadata": {},
   "source": [
    "### 희소 표현(Sparse Representation) | 분산 표현(Distributed Representation)\n",
    "- 원-핫 인코딩으로 얻은 원-핫 벡터는 단어의 인덱스 값만 1이고 나머지는 모두 0으로 표현된다.\n",
    "- 이렇게 대부분의 값이 0인 벡터나 행렬을 사용하는 표현 방식을 희소 표현(sparse representation)이라고 한다.  \n",
    "- 희소 표현은 단어 벡터 간 유의미한 유사성을 표현할 수 없다는 단점이 있다.\n",
    "- 이를 해결하기 위해 단어의 의미를 다차원 공간에 벡터화하는 분산 표현(distributed representation)을 사용한다.\n",
    "- 분산 표현으로 단어 간 의미적 유사성을 벡터화하는 작업을 워드 임베딩(embedding)이라고 하며, 이렇게 변환된 벡터를 임베딩 벡터(embedding vector)라고 한다.  \n",
    "- **원-핫 인코딩 → 희소 표현**  \n",
    "- **워드 임베딩 → 분산 표현**  \n",
    "\n",
    "**분산 표현(Distributed Representation)**\n",
    "- 분산 표현은 분포 가설(distributional hypothesis)에 기반한 방법이다.\n",
    "- 이 가설은 \"비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다\"는 내용을 전제로 한다.\n",
    "- 예를 들어, '강아지'라는 단어는 '귀엽다', '예쁘다', '애교' 등의 단어와 함께 자주 등장하며, 이를 벡터화하면 해당 단어들은 유사한 벡터값을 갖게 된다.\n",
    "- 분산 표현은 단어의 의미를 여러 차원에 걸쳐 분산하여 표현한다.  \n",
    "- 이 방식은 원-핫 벡터처럼 단어 집합 크기만큼의 차원이 필요하지 않으며, 상대적으로 저차원으로 줄어든다.\n",
    "- 예를 들어, 단어 집합 크기가 10,000이고 '강아지'의 인덱스가 4라면, 원-핫 벡터는 다음과 같다:\n",
    "  \n",
    "- **강아지 = [0 0 0 0 1 0 0 ... 0]** (뒤에 9,995개의 0 포함)  \n",
    "- 그러나 Word2Vec으로 임베딩된 벡터는 단어 집합 크기와 무관하며, 설정된 차원의 수만큼 실수값을 가진 벡터가 된다:  \n",
    "- **강아지 = [0.2 0.3 0.5 0.7 0.2 ... 0.2]**  \n",
    "\n",
    "**요약하면,**\n",
    "- 희소 표현은 고차원에서 각 차원이 분리된 방식으로 단어를 표현하지만, 분산 표현은 저차원에서 단어의 의미를 여러 차원에 분산시켜 표현한다.\n",
    "- 이를 통해 단어 벡터 간 유의미한 유사도를 계산할 수 있으며, 대표적인 학습 방법으로 Word2Vec이 사용된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0fea6",
   "metadata": {},
   "source": [
    "### Embedding Vector 시각화 wevi\n",
    "https://ronxin.github.io/wevi/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87462405",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 2013년 구글에서 개발한 Word Embedding 방법\n",
    "- 최초의 neural embedding model\n",
    "- 매우 큰 corpus에서 자동 학습\n",
    "    - 비지도 지도 학습 (자기 지도학습)이라 할 수 있음\n",
    "    - 많은 데이터를 기반으로 label 값 유추하고 이를 지도학습에 사용\n",
    "- ex) \n",
    "    - **이사금**께 충성을 맹세하였다.\n",
    "    - **왕**께 충성을 맹세하였다.\n",
    "\n",
    "**WordVec 훈련방식에 따른 구분**\n",
    "1. CBOW : 주변 단어로 중심 단어를 예측 (벡터값 유추)\n",
    "2. Skip-gram : 중심 단어로 주변 단어를 예측 (벡터값 유추)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22a0a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비지도 지도학습(자가지도학습) : 비지도처럼 정답이 없는 데이터를 이용하지만, 그 안에서 스스로 ‘가짜 정답(라벨)’을 만들어서 지도학습처럼 학습하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a6c2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에서 일부 단어를 가리고, 그 가려진 단어를 맞히면서 문맥을 이해하는 능력을 학습하는 방법 (문장 속 단어 몇 개를 가려놓고, 그 가려진 단어가 뭔지 맞히게 학습시키는 방식)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03832c58",
   "metadata": {},
   "source": [
    "##### CBOW (Continuous Bag of Words)  \n",
    "- CBOW는 원-핫 벡터를 사용하지만, 이는 단순히 위치를 가리킬 뿐 vocabulary를 직접적으로 참조하지 않는다.  \n",
    "\n",
    "**예시:**  \n",
    "\n",
    "> The fat cat sat on the mat  \n",
    "\n",
    "주어진 문장에서 'sat'이라는 단어를 예측하는 것이 CBOW의 주요 작업이다.  \n",
    "- **중심 단어(center word):** 예측하려는 단어 ('sat')  \n",
    "- **주변 단어(context word):** 예측에 사용되는 단어들  \n",
    "\n",
    "중심 단어를 예측하기 위해 앞뒤 몇 개의 단어를 참고할지 결정하는 범위를 **윈도우(window)**라고 한다.  \n",
    "예를 들어, 윈도우 크기가 2이고 중심 단어가 'sat'라면, 앞의 두 단어(fat, cat)와 뒤의 두 단어(on, the)를 입력으로 사용한다.  \n",
    "윈도우 크기가 n일 경우, 참고하는 주변 단어의 개수는 총 2n이다. 윈도우를 옆으로 이동하며 학습 데이터를 생성하는 방법을 **슬라이딩 윈도우(sliding window)**라고 한다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)\n",
    "\n",
    "\n",
    "**훈련 과정**\n",
    "\n",
    "CBOW는 embedding 벡터를 학습하기 위한 구조를 갖는다. 초기에는 가중치가 임의의 값으로 설정되며, 역전파를 통해 최적화된다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)\n",
    "\n",
    "Word2Vec은 은닉층이 하나뿐인 얕은 신경망(shallow neural network) 구조를 사용한다.  \n",
    "학습 대상이 되는 주요 가중치는 두 가지이다:  \n",
    "\n",
    "1. **투사층(projection layer):**  \n",
    "   - 활성화 함수가 없으며 룩업 테이블 연산을 담당한다.  \n",
    "   - 입력층과 투사층 사이의 가중치 W는 V × M 행렬로 표현되며, 여기서 **V는 단어 집합의 크기, M은 벡터의 차원**이다.  \n",
    "   - W 행렬의 각 행은 학습 후 단어의 M차원 임베딩 벡터로 간주된다.  \n",
    "   - 예를 들어, 벡터 차원을 5로 설정하면 각 단어의 임베딩 벡터는 5차원이 된다.  \n",
    "\n",
    "2. **출력층:**  \n",
    "   - 투사층과 출력층 사이의 가중치 W'는 M × V 행렬로 표현된다.  \n",
    "   - 이 두 행렬(W와 W')은 서로 독립적이며, 학습 전에는 랜덤 값으로 초기화된다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG)\n",
    "\n",
    "\n",
    "**예측 과정**\n",
    "1. CBOW는 계산된 룩업 테이블의 평균을 구한 뒤, 출력층의 가중치 W'와 내적한다.  \n",
    "2. 결과값은 **소프트맥스(softmax)** 활성화 함수에 입력되어, 중심 단어일 확률을 나타내는 예측값으로 변환된다.  \n",
    "3. 출력된 예측값(스코어 벡터)은 실제 타겟 원-핫 벡터와 비교되며, **크로스 엔트로피(cross-entropy)** 함수로 손실값을 계산한다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_5.PNG)\n",
    "\n",
    "손실 함수 식:  \n",
    "$\n",
    "cost(\\hat{y}, y) = -\\sum_{j=1}^{V} y_{j} \\cdot log(\\hat{y}_{j})\n",
    "$  \n",
    "\n",
    "여기서, $\\hat{y}_{j}$는 예측 확률, $y_{j}$는 실제 값이며, V는 단어 집합의 크기를 의미한다.  \n",
    "\n",
    "\n",
    "**학습 결과**  \n",
    "- 역전파를 통해 가중치 W와 W'가 학습된다. \n",
    "- 학습이 완료되면 W 행렬의 각 행을 단어의 임베딩 벡터로 사용하거나, W와 W' 모두를 이용해 임베딩 벡터를 생성할 수 있다.  \n",
    "- CBOW는 주변 단어를 기반으로 중심 단어를 예측하는 구조를 갖추고 있으며, 이를 통해 단어 간 의미적 관계를 효과적으로 학습할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc2515",
   "metadata": {},
   "source": [
    "##### Skip-gram\n",
    "- Skip-gram은 중심 단어에서 주변 단어를 예측한다.\n",
    "- 윈도우 크기가 2일 때, 데이터셋은 다음과 같이 구성된다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/skipgram_dataset.PNG)\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)\n",
    "\n",
    "- 중심 단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없다.\n",
    "- 여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bebccf",
   "metadata": {},
   "source": [
    "https://regexr.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e2b23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim   # word2vec을 지원해주고 있는 라이브러리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1aa99a",
   "metadata": {},
   "source": [
    "##### 영어 Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9636fd5",
   "metadata": {},
   "source": [
    "- 데이터 취득 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a7fe0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1TF1yAHF3qRINbXWFOajFjUCxUF64QZMX\n",
      "From (redirected): https://drive.google.com/uc?id=1TF1yAHF3qRINbXWFOajFjUCxUF64QZMX&confirm=t&uuid=bb923696-7901-4e2e-8438-d24c210d565b\n",
      "To: c:\\encore_skn11\\07_nlp\\03_word_embedding\\ted_en.xml\n",
      "100%|██████████| 74.5M/74.5M [00:01<00:00, 40.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ted_en.xml'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1TF1yAHF3qRINbXWFOajFjUCxUF64QZMX'\n",
    "output = 'ted_en.xml'\n",
    "\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b0f8828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a78cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24222849\n",
      "24062319\n"
     ]
    }
   ],
   "source": [
    "# xml 데이터 처리\n",
    "f = open('ted_en.xml', 'r', encoding='UTF-8')\n",
    "xml = etree.parse(f)\n",
    "\n",
    "contents = xml.xpath('//content/text()')    # content 태그 하위 텍슽 \n",
    "# contents[:5]\n",
    "\n",
    "corpus = '\\n'.join(contents)\n",
    "print(len(corpus))\n",
    "\n",
    "# 정규식을 이용해 (Laughter), (Applause) 등 키워드 제거  \n",
    "corpus = re.sub(r'\\([^)]*\\)', '', corpus)  \n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f52cf850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two', 'reasons', 'companies', 'fail', 'new'],\n",
       " ['real',\n",
       "  'real',\n",
       "  'solution',\n",
       "  'quality',\n",
       "  'growth',\n",
       "  'figuring',\n",
       "  'balance',\n",
       "  'two',\n",
       "  'activities',\n",
       "  'exploration',\n",
       "  'exploitation'],\n",
       " ['necessary', 'much', 'good', 'thing'],\n",
       " ['consider', 'facit'],\n",
       " ['actually', 'old', 'enough', 'remember']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 (토큰화/대소문자 정규화/불용어 처리)\n",
    "sentences = sent_tokenize(corpus)\n",
    "preprocessed_sentences = []\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-z0-9]', ' ', sentence)  # 영소문자, 숫자 외 제거 \n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [token for token in tokens if token not in en_stopwords]\n",
    "    preprocessed_sentences.append(tokens)\n",
    "\n",
    "preprocessed_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e20e06",
   "metadata": {},
   "source": [
    "- Embedding 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b3bbc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21462, 100)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=preprocessed_sentences,   # corpus\n",
    "    vector_size=100,                    # embedding vector의 차원\n",
    "    sg=0,                               # 학습 알고리즘 선택 (1을 넣으면 Skip-gram, 0을 넣으면 CBOW 학습 알고리즘을 선택)\n",
    "    window=5,                           # 중심단어 주위의 주변단어로서 사용될 개수 (앞뒤로 n개 고려)\n",
    "    min_count=5                         # 최소 빈도수 (빈도수가 5보다 작으면 제거)\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05492dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>-1.037543</td>\n",
       "      <td>0.256925</td>\n",
       "      <td>-0.456524</td>\n",
       "      <td>-0.051298</td>\n",
       "      <td>0.097585</td>\n",
       "      <td>-0.966302</td>\n",
       "      <td>-0.565377</td>\n",
       "      <td>0.368886</td>\n",
       "      <td>-1.929277</td>\n",
       "      <td>-0.312155</td>\n",
       "      <td>0.985646</td>\n",
       "      <td>0.283635</td>\n",
       "      <td>-0.439902</td>\n",
       "      <td>0.404080</td>\n",
       "      <td>0.278465</td>\n",
       "      <td>-0.432822</td>\n",
       "      <td>0.335190</td>\n",
       "      <td>-0.855497</td>\n",
       "      <td>-0.898538</td>\n",
       "      <td>-1.666382</td>\n",
       "      <td>1.503753</td>\n",
       "      <td>0.282579</td>\n",
       "      <td>1.233055</td>\n",
       "      <td>-0.711129</td>\n",
       "      <td>0.066786</td>\n",
       "      <td>-0.922890</td>\n",
       "      <td>0.114391</td>\n",
       "      <td>-0.076930</td>\n",
       "      <td>0.382532</td>\n",
       "      <td>-0.309766</td>\n",
       "      <td>-0.341735</td>\n",
       "      <td>0.777135</td>\n",
       "      <td>0.286894</td>\n",
       "      <td>-0.980891</td>\n",
       "      <td>-0.066363</td>\n",
       "      <td>0.540996</td>\n",
       "      <td>-0.048148</td>\n",
       "      <td>-0.064269</td>\n",
       "      <td>-0.326356</td>\n",
       "      <td>-1.481685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792139</td>\n",
       "      <td>0.442927</td>\n",
       "      <td>-1.040028</td>\n",
       "      <td>-0.302952</td>\n",
       "      <td>0.106998</td>\n",
       "      <td>0.361419</td>\n",
       "      <td>0.096842</td>\n",
       "      <td>0.079683</td>\n",
       "      <td>-0.219651</td>\n",
       "      <td>0.504311</td>\n",
       "      <td>-0.328717</td>\n",
       "      <td>0.169539</td>\n",
       "      <td>0.149502</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>-0.451252</td>\n",
       "      <td>-1.259105</td>\n",
       "      <td>-0.056237</td>\n",
       "      <td>0.111750</td>\n",
       "      <td>0.059826</td>\n",
       "      <td>1.108103</td>\n",
       "      <td>0.916090</td>\n",
       "      <td>0.542874</td>\n",
       "      <td>-0.990058</td>\n",
       "      <td>-0.458527</td>\n",
       "      <td>-1.918749</td>\n",
       "      <td>-0.736803</td>\n",
       "      <td>-0.774773</td>\n",
       "      <td>-0.086868</td>\n",
       "      <td>-0.465951</td>\n",
       "      <td>2.017634</td>\n",
       "      <td>1.418058</td>\n",
       "      <td>0.450860</td>\n",
       "      <td>-0.933226</td>\n",
       "      <td>-0.133593</td>\n",
       "      <td>0.724407</td>\n",
       "      <td>-1.697631</td>\n",
       "      <td>-1.307543</td>\n",
       "      <td>0.098816</td>\n",
       "      <td>1.096606</td>\n",
       "      <td>1.468806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>-1.624187</td>\n",
       "      <td>0.492345</td>\n",
       "      <td>-0.033345</td>\n",
       "      <td>0.237822</td>\n",
       "      <td>-0.694842</td>\n",
       "      <td>-0.407227</td>\n",
       "      <td>-0.084756</td>\n",
       "      <td>0.409875</td>\n",
       "      <td>-0.759424</td>\n",
       "      <td>-2.812324</td>\n",
       "      <td>1.183966</td>\n",
       "      <td>-0.271538</td>\n",
       "      <td>0.364884</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>1.663470</td>\n",
       "      <td>-0.467956</td>\n",
       "      <td>0.383573</td>\n",
       "      <td>0.507944</td>\n",
       "      <td>0.226170</td>\n",
       "      <td>0.165070</td>\n",
       "      <td>0.774358</td>\n",
       "      <td>-1.994244</td>\n",
       "      <td>3.106387</td>\n",
       "      <td>0.459523</td>\n",
       "      <td>-0.296938</td>\n",
       "      <td>0.068420</td>\n",
       "      <td>-0.816073</td>\n",
       "      <td>-0.857725</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>-0.018559</td>\n",
       "      <td>0.766922</td>\n",
       "      <td>0.355904</td>\n",
       "      <td>-0.148447</td>\n",
       "      <td>0.896573</td>\n",
       "      <td>-0.138577</td>\n",
       "      <td>1.520363</td>\n",
       "      <td>0.699693</td>\n",
       "      <td>-0.734261</td>\n",
       "      <td>-0.818657</td>\n",
       "      <td>1.123982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803025</td>\n",
       "      <td>-0.032620</td>\n",
       "      <td>-0.350681</td>\n",
       "      <td>-1.418500</td>\n",
       "      <td>-0.619981</td>\n",
       "      <td>0.490477</td>\n",
       "      <td>0.611523</td>\n",
       "      <td>0.065134</td>\n",
       "      <td>0.397060</td>\n",
       "      <td>1.956630</td>\n",
       "      <td>0.962901</td>\n",
       "      <td>0.404418</td>\n",
       "      <td>0.652151</td>\n",
       "      <td>0.324194</td>\n",
       "      <td>-0.513917</td>\n",
       "      <td>2.069163</td>\n",
       "      <td>-0.977550</td>\n",
       "      <td>-0.205800</td>\n",
       "      <td>0.381311</td>\n",
       "      <td>-0.616677</td>\n",
       "      <td>-2.694984</td>\n",
       "      <td>-0.257644</td>\n",
       "      <td>0.096314</td>\n",
       "      <td>0.586033</td>\n",
       "      <td>-1.093633</td>\n",
       "      <td>1.139184</td>\n",
       "      <td>-0.951657</td>\n",
       "      <td>-0.875583</td>\n",
       "      <td>-0.878273</td>\n",
       "      <td>0.422534</td>\n",
       "      <td>1.163053</td>\n",
       "      <td>1.030891</td>\n",
       "      <td>-1.037176</td>\n",
       "      <td>-0.815198</td>\n",
       "      <td>-0.723342</td>\n",
       "      <td>0.300006</td>\n",
       "      <td>0.148611</td>\n",
       "      <td>-0.626744</td>\n",
       "      <td>-1.626490</td>\n",
       "      <td>1.004843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>-0.868234</td>\n",
       "      <td>-0.626363</td>\n",
       "      <td>-0.779085</td>\n",
       "      <td>-0.609282</td>\n",
       "      <td>-0.121492</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.212054</td>\n",
       "      <td>0.368738</td>\n",
       "      <td>-1.920715</td>\n",
       "      <td>0.784323</td>\n",
       "      <td>-1.361491</td>\n",
       "      <td>0.273666</td>\n",
       "      <td>0.697903</td>\n",
       "      <td>-0.063869</td>\n",
       "      <td>1.657494</td>\n",
       "      <td>-0.895221</td>\n",
       "      <td>-0.091522</td>\n",
       "      <td>0.403542</td>\n",
       "      <td>-0.055418</td>\n",
       "      <td>0.411376</td>\n",
       "      <td>0.373937</td>\n",
       "      <td>-0.827234</td>\n",
       "      <td>-0.893167</td>\n",
       "      <td>-1.812549</td>\n",
       "      <td>0.670303</td>\n",
       "      <td>-0.312897</td>\n",
       "      <td>2.486383</td>\n",
       "      <td>0.731049</td>\n",
       "      <td>-0.667016</td>\n",
       "      <td>0.052039</td>\n",
       "      <td>1.289160</td>\n",
       "      <td>-0.590186</td>\n",
       "      <td>-0.934933</td>\n",
       "      <td>0.026615</td>\n",
       "      <td>-1.478675</td>\n",
       "      <td>-0.106513</td>\n",
       "      <td>0.520368</td>\n",
       "      <td>1.107007</td>\n",
       "      <td>-0.183793</td>\n",
       "      <td>-0.254251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828883</td>\n",
       "      <td>-0.598797</td>\n",
       "      <td>0.976088</td>\n",
       "      <td>-0.557595</td>\n",
       "      <td>-1.175766</td>\n",
       "      <td>2.331516</td>\n",
       "      <td>0.684848</td>\n",
       "      <td>-0.775479</td>\n",
       "      <td>-0.931835</td>\n",
       "      <td>0.281739</td>\n",
       "      <td>0.926357</td>\n",
       "      <td>0.553959</td>\n",
       "      <td>1.117526</td>\n",
       "      <td>-0.252998</td>\n",
       "      <td>0.341684</td>\n",
       "      <td>1.741089</td>\n",
       "      <td>1.889227</td>\n",
       "      <td>-0.213646</td>\n",
       "      <td>-0.578799</td>\n",
       "      <td>0.730605</td>\n",
       "      <td>-0.032705</td>\n",
       "      <td>-0.198634</td>\n",
       "      <td>-1.818456</td>\n",
       "      <td>0.788169</td>\n",
       "      <td>0.288374</td>\n",
       "      <td>-1.001983</td>\n",
       "      <td>-1.239816</td>\n",
       "      <td>0.278730</td>\n",
       "      <td>0.821540</td>\n",
       "      <td>0.111610</td>\n",
       "      <td>-0.176924</td>\n",
       "      <td>0.236012</td>\n",
       "      <td>0.292788</td>\n",
       "      <td>-0.369551</td>\n",
       "      <td>-0.428780</td>\n",
       "      <td>0.866908</td>\n",
       "      <td>-0.207208</td>\n",
       "      <td>-0.160890</td>\n",
       "      <td>0.909012</td>\n",
       "      <td>-1.139922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>-0.730419</td>\n",
       "      <td>-0.223894</td>\n",
       "      <td>-0.771613</td>\n",
       "      <td>0.589559</td>\n",
       "      <td>0.273112</td>\n",
       "      <td>-0.376140</td>\n",
       "      <td>-0.063493</td>\n",
       "      <td>0.116479</td>\n",
       "      <td>-0.388547</td>\n",
       "      <td>-0.552971</td>\n",
       "      <td>-0.305082</td>\n",
       "      <td>-0.428757</td>\n",
       "      <td>-0.311401</td>\n",
       "      <td>-0.442544</td>\n",
       "      <td>0.727279</td>\n",
       "      <td>0.086084</td>\n",
       "      <td>0.789956</td>\n",
       "      <td>0.366774</td>\n",
       "      <td>0.117284</td>\n",
       "      <td>-1.048147</td>\n",
       "      <td>0.342976</td>\n",
       "      <td>-0.011824</td>\n",
       "      <td>0.322511</td>\n",
       "      <td>0.581102</td>\n",
       "      <td>0.480408</td>\n",
       "      <td>0.867339</td>\n",
       "      <td>0.964122</td>\n",
       "      <td>-0.400571</td>\n",
       "      <td>-0.324457</td>\n",
       "      <td>0.006519</td>\n",
       "      <td>0.462021</td>\n",
       "      <td>0.055638</td>\n",
       "      <td>-0.513282</td>\n",
       "      <td>-0.192810</td>\n",
       "      <td>-0.347237</td>\n",
       "      <td>0.546274</td>\n",
       "      <td>0.344883</td>\n",
       "      <td>-0.887153</td>\n",
       "      <td>-0.286458</td>\n",
       "      <td>-0.681864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>-0.161763</td>\n",
       "      <td>0.465145</td>\n",
       "      <td>-0.670182</td>\n",
       "      <td>-0.972113</td>\n",
       "      <td>0.790358</td>\n",
       "      <td>0.420402</td>\n",
       "      <td>0.912053</td>\n",
       "      <td>-1.478917</td>\n",
       "      <td>-0.493234</td>\n",
       "      <td>0.638826</td>\n",
       "      <td>0.930349</td>\n",
       "      <td>0.295155</td>\n",
       "      <td>0.509309</td>\n",
       "      <td>0.871249</td>\n",
       "      <td>-0.259527</td>\n",
       "      <td>-0.343176</td>\n",
       "      <td>0.163859</td>\n",
       "      <td>-0.866958</td>\n",
       "      <td>0.350930</td>\n",
       "      <td>-0.922148</td>\n",
       "      <td>0.139717</td>\n",
       "      <td>-1.530186</td>\n",
       "      <td>0.657039</td>\n",
       "      <td>-1.020263</td>\n",
       "      <td>0.479910</td>\n",
       "      <td>0.841832</td>\n",
       "      <td>1.167693</td>\n",
       "      <td>0.550820</td>\n",
       "      <td>-0.993412</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>0.391151</td>\n",
       "      <td>-0.112773</td>\n",
       "      <td>-0.470254</td>\n",
       "      <td>-0.662986</td>\n",
       "      <td>0.079997</td>\n",
       "      <td>-0.005156</td>\n",
       "      <td>-0.466870</td>\n",
       "      <td>0.795719</td>\n",
       "      <td>-0.620436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>-1.217457</td>\n",
       "      <td>0.998311</td>\n",
       "      <td>-0.800044</td>\n",
       "      <td>-0.558765</td>\n",
       "      <td>1.854455</td>\n",
       "      <td>0.207791</td>\n",
       "      <td>-0.531749</td>\n",
       "      <td>1.331990</td>\n",
       "      <td>-0.515191</td>\n",
       "      <td>-0.887948</td>\n",
       "      <td>-0.847039</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.236947</td>\n",
       "      <td>0.341424</td>\n",
       "      <td>0.872574</td>\n",
       "      <td>-1.281547</td>\n",
       "      <td>0.220324</td>\n",
       "      <td>0.553818</td>\n",
       "      <td>0.934403</td>\n",
       "      <td>-1.903590</td>\n",
       "      <td>-0.155387</td>\n",
       "      <td>-1.477983</td>\n",
       "      <td>-0.089042</td>\n",
       "      <td>1.551849</td>\n",
       "      <td>0.196964</td>\n",
       "      <td>-0.448708</td>\n",
       "      <td>-0.386514</td>\n",
       "      <td>0.339032</td>\n",
       "      <td>0.452127</td>\n",
       "      <td>-0.413506</td>\n",
       "      <td>1.200090</td>\n",
       "      <td>1.561357</td>\n",
       "      <td>0.691826</td>\n",
       "      <td>-0.068079</td>\n",
       "      <td>0.675684</td>\n",
       "      <td>-0.223103</td>\n",
       "      <td>-0.269838</td>\n",
       "      <td>0.020678</td>\n",
       "      <td>-0.414629</td>\n",
       "      <td>0.397458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322329</td>\n",
       "      <td>1.212499</td>\n",
       "      <td>0.939324</td>\n",
       "      <td>0.555845</td>\n",
       "      <td>-1.513418</td>\n",
       "      <td>0.964989</td>\n",
       "      <td>0.357831</td>\n",
       "      <td>1.376425</td>\n",
       "      <td>-0.983460</td>\n",
       "      <td>-0.457857</td>\n",
       "      <td>-1.527039</td>\n",
       "      <td>1.028238</td>\n",
       "      <td>-0.319057</td>\n",
       "      <td>0.919059</td>\n",
       "      <td>0.678590</td>\n",
       "      <td>1.716021</td>\n",
       "      <td>0.942217</td>\n",
       "      <td>1.425267</td>\n",
       "      <td>-1.023851</td>\n",
       "      <td>0.364298</td>\n",
       "      <td>-0.254255</td>\n",
       "      <td>-0.475094</td>\n",
       "      <td>-0.697739</td>\n",
       "      <td>-0.833883</td>\n",
       "      <td>-0.856777</td>\n",
       "      <td>1.166101</td>\n",
       "      <td>0.981448</td>\n",
       "      <td>0.222386</td>\n",
       "      <td>-1.113844</td>\n",
       "      <td>0.347084</td>\n",
       "      <td>0.524775</td>\n",
       "      <td>-0.602731</td>\n",
       "      <td>-0.432099</td>\n",
       "      <td>1.684758</td>\n",
       "      <td>0.283094</td>\n",
       "      <td>-0.546439</td>\n",
       "      <td>1.327456</td>\n",
       "      <td>-0.315375</td>\n",
       "      <td>-0.844938</td>\n",
       "      <td>0.604256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>-0.698013</td>\n",
       "      <td>-0.350541</td>\n",
       "      <td>0.439105</td>\n",
       "      <td>-0.423330</td>\n",
       "      <td>-0.002902</td>\n",
       "      <td>-0.713102</td>\n",
       "      <td>0.327641</td>\n",
       "      <td>0.415620</td>\n",
       "      <td>-0.558601</td>\n",
       "      <td>-0.681224</td>\n",
       "      <td>-0.027001</td>\n",
       "      <td>-0.239665</td>\n",
       "      <td>0.193703</td>\n",
       "      <td>-1.084151</td>\n",
       "      <td>-0.152699</td>\n",
       "      <td>-0.991992</td>\n",
       "      <td>-0.310680</td>\n",
       "      <td>-0.558592</td>\n",
       "      <td>0.166604</td>\n",
       "      <td>-0.305133</td>\n",
       "      <td>-0.781597</td>\n",
       "      <td>0.434186</td>\n",
       "      <td>1.815184</td>\n",
       "      <td>0.621272</td>\n",
       "      <td>0.554223</td>\n",
       "      <td>0.064271</td>\n",
       "      <td>-0.039505</td>\n",
       "      <td>-0.239199</td>\n",
       "      <td>0.447448</td>\n",
       "      <td>0.558048</td>\n",
       "      <td>0.337707</td>\n",
       "      <td>0.330063</td>\n",
       "      <td>0.839116</td>\n",
       "      <td>-0.221557</td>\n",
       "      <td>-0.168948</td>\n",
       "      <td>1.368480</td>\n",
       "      <td>0.748682</td>\n",
       "      <td>0.195159</td>\n",
       "      <td>0.482072</td>\n",
       "      <td>-1.026534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081276</td>\n",
       "      <td>0.401096</td>\n",
       "      <td>1.035606</td>\n",
       "      <td>-0.247496</td>\n",
       "      <td>-1.257211</td>\n",
       "      <td>0.892749</td>\n",
       "      <td>1.143865</td>\n",
       "      <td>-0.093701</td>\n",
       "      <td>-1.885954</td>\n",
       "      <td>-0.333048</td>\n",
       "      <td>0.900790</td>\n",
       "      <td>1.471134</td>\n",
       "      <td>0.611164</td>\n",
       "      <td>0.832383</td>\n",
       "      <td>0.666187</td>\n",
       "      <td>-0.225479</td>\n",
       "      <td>-0.044388</td>\n",
       "      <td>0.153181</td>\n",
       "      <td>-0.071480</td>\n",
       "      <td>0.287535</td>\n",
       "      <td>-0.585079</td>\n",
       "      <td>-0.035646</td>\n",
       "      <td>-0.987847</td>\n",
       "      <td>-0.225045</td>\n",
       "      <td>-0.679795</td>\n",
       "      <td>0.318586</td>\n",
       "      <td>0.175872</td>\n",
       "      <td>1.545713</td>\n",
       "      <td>0.192996</td>\n",
       "      <td>-0.120158</td>\n",
       "      <td>1.363064</td>\n",
       "      <td>1.348718</td>\n",
       "      <td>-0.199634</td>\n",
       "      <td>0.169713</td>\n",
       "      <td>0.251029</td>\n",
       "      <td>-0.326358</td>\n",
       "      <td>-0.393013</td>\n",
       "      <td>-1.129160</td>\n",
       "      <td>-0.103626</td>\n",
       "      <td>-0.687824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.011837</td>\n",
       "      <td>-0.309356</td>\n",
       "      <td>0.814826</td>\n",
       "      <td>-0.893165</td>\n",
       "      <td>-0.756341</td>\n",
       "      <td>-0.831024</td>\n",
       "      <td>-0.533742</td>\n",
       "      <td>0.551236</td>\n",
       "      <td>-1.732524</td>\n",
       "      <td>0.980099</td>\n",
       "      <td>-0.524586</td>\n",
       "      <td>-0.111244</td>\n",
       "      <td>-0.283461</td>\n",
       "      <td>0.362067</td>\n",
       "      <td>0.524442</td>\n",
       "      <td>-1.457345</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>-0.227996</td>\n",
       "      <td>1.078211</td>\n",
       "      <td>-0.944182</td>\n",
       "      <td>0.291360</td>\n",
       "      <td>-0.583276</td>\n",
       "      <td>-0.561987</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>1.333853</td>\n",
       "      <td>0.514647</td>\n",
       "      <td>0.642655</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>0.114323</td>\n",
       "      <td>0.840784</td>\n",
       "      <td>0.049981</td>\n",
       "      <td>0.986045</td>\n",
       "      <td>0.107874</td>\n",
       "      <td>0.631564</td>\n",
       "      <td>0.363832</td>\n",
       "      <td>1.531346</td>\n",
       "      <td>-0.794771</td>\n",
       "      <td>-2.028866</td>\n",
       "      <td>0.279526</td>\n",
       "      <td>-0.837822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449252</td>\n",
       "      <td>-0.185145</td>\n",
       "      <td>-0.676833</td>\n",
       "      <td>1.000013</td>\n",
       "      <td>-0.433591</td>\n",
       "      <td>-0.496693</td>\n",
       "      <td>-0.112013</td>\n",
       "      <td>-0.708001</td>\n",
       "      <td>-0.923223</td>\n",
       "      <td>0.418192</td>\n",
       "      <td>-0.147092</td>\n",
       "      <td>0.373842</td>\n",
       "      <td>0.181920</td>\n",
       "      <td>0.050845</td>\n",
       "      <td>0.491707</td>\n",
       "      <td>-0.697894</td>\n",
       "      <td>0.771469</td>\n",
       "      <td>0.341502</td>\n",
       "      <td>-0.398357</td>\n",
       "      <td>0.812333</td>\n",
       "      <td>0.103181</td>\n",
       "      <td>0.507074</td>\n",
       "      <td>-0.809158</td>\n",
       "      <td>0.452946</td>\n",
       "      <td>0.616222</td>\n",
       "      <td>1.505453</td>\n",
       "      <td>0.373915</td>\n",
       "      <td>1.150921</td>\n",
       "      <td>0.372524</td>\n",
       "      <td>0.197352</td>\n",
       "      <td>-0.345581</td>\n",
       "      <td>0.676969</td>\n",
       "      <td>0.318996</td>\n",
       "      <td>0.219935</td>\n",
       "      <td>0.429069</td>\n",
       "      <td>0.235492</td>\n",
       "      <td>0.635551</td>\n",
       "      <td>-0.984520</td>\n",
       "      <td>0.250803</td>\n",
       "      <td>-0.373136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>-0.314115</td>\n",
       "      <td>0.284673</td>\n",
       "      <td>1.002277</td>\n",
       "      <td>-0.718731</td>\n",
       "      <td>1.877864</td>\n",
       "      <td>0.647009</td>\n",
       "      <td>-0.391647</td>\n",
       "      <td>0.199008</td>\n",
       "      <td>-1.867468</td>\n",
       "      <td>-0.364277</td>\n",
       "      <td>-0.278587</td>\n",
       "      <td>0.040899</td>\n",
       "      <td>-0.007944</td>\n",
       "      <td>0.702940</td>\n",
       "      <td>-0.064164</td>\n",
       "      <td>-0.446113</td>\n",
       "      <td>0.474226</td>\n",
       "      <td>1.155997</td>\n",
       "      <td>-0.198069</td>\n",
       "      <td>-0.809132</td>\n",
       "      <td>0.296678</td>\n",
       "      <td>-0.346320</td>\n",
       "      <td>-0.127738</td>\n",
       "      <td>-0.852389</td>\n",
       "      <td>-1.028188</td>\n",
       "      <td>-0.699171</td>\n",
       "      <td>-0.983645</td>\n",
       "      <td>-0.342364</td>\n",
       "      <td>-0.832245</td>\n",
       "      <td>-0.008550</td>\n",
       "      <td>0.273549</td>\n",
       "      <td>1.094226</td>\n",
       "      <td>-0.318130</td>\n",
       "      <td>-0.862018</td>\n",
       "      <td>0.077014</td>\n",
       "      <td>0.912072</td>\n",
       "      <td>-1.730280</td>\n",
       "      <td>0.084342</td>\n",
       "      <td>-1.580748</td>\n",
       "      <td>1.797623</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943131</td>\n",
       "      <td>0.692836</td>\n",
       "      <td>-1.499373</td>\n",
       "      <td>-0.326570</td>\n",
       "      <td>0.072018</td>\n",
       "      <td>0.535699</td>\n",
       "      <td>-0.300201</td>\n",
       "      <td>1.492770</td>\n",
       "      <td>-0.618633</td>\n",
       "      <td>-0.928613</td>\n",
       "      <td>1.319510</td>\n",
       "      <td>0.179915</td>\n",
       "      <td>0.722216</td>\n",
       "      <td>-0.468315</td>\n",
       "      <td>-0.292347</td>\n",
       "      <td>2.203519</td>\n",
       "      <td>2.158412</td>\n",
       "      <td>0.163281</td>\n",
       "      <td>0.565245</td>\n",
       "      <td>-0.836462</td>\n",
       "      <td>-1.206817</td>\n",
       "      <td>0.125269</td>\n",
       "      <td>-2.132327</td>\n",
       "      <td>-1.193671</td>\n",
       "      <td>-0.264488</td>\n",
       "      <td>0.483963</td>\n",
       "      <td>1.482911</td>\n",
       "      <td>0.709435</td>\n",
       "      <td>1.387210</td>\n",
       "      <td>0.314114</td>\n",
       "      <td>0.930601</td>\n",
       "      <td>-0.612731</td>\n",
       "      <td>-0.553153</td>\n",
       "      <td>1.390393</td>\n",
       "      <td>-0.459028</td>\n",
       "      <td>0.513495</td>\n",
       "      <td>-0.104244</td>\n",
       "      <td>-0.993600</td>\n",
       "      <td>-0.702535</td>\n",
       "      <td>-1.063961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>-2.165626</td>\n",
       "      <td>-1.054550</td>\n",
       "      <td>-0.125160</td>\n",
       "      <td>0.534360</td>\n",
       "      <td>0.532016</td>\n",
       "      <td>0.173083</td>\n",
       "      <td>1.108847</td>\n",
       "      <td>0.669265</td>\n",
       "      <td>-0.679644</td>\n",
       "      <td>-1.174095</td>\n",
       "      <td>0.622346</td>\n",
       "      <td>-1.201398</td>\n",
       "      <td>1.226960</td>\n",
       "      <td>0.449152</td>\n",
       "      <td>0.399258</td>\n",
       "      <td>-0.542976</td>\n",
       "      <td>-0.269319</td>\n",
       "      <td>-0.370091</td>\n",
       "      <td>0.767597</td>\n",
       "      <td>-1.210803</td>\n",
       "      <td>-0.240126</td>\n",
       "      <td>0.092191</td>\n",
       "      <td>1.307999</td>\n",
       "      <td>0.267733</td>\n",
       "      <td>1.551126</td>\n",
       "      <td>1.068226</td>\n",
       "      <td>-0.266986</td>\n",
       "      <td>-1.865929</td>\n",
       "      <td>-0.000936</td>\n",
       "      <td>-0.182694</td>\n",
       "      <td>1.552540</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>1.899784</td>\n",
       "      <td>0.758373</td>\n",
       "      <td>-0.508254</td>\n",
       "      <td>0.807865</td>\n",
       "      <td>0.571889</td>\n",
       "      <td>0.416567</td>\n",
       "      <td>-0.080914</td>\n",
       "      <td>-0.735670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434285</td>\n",
       "      <td>-0.455447</td>\n",
       "      <td>2.099265</td>\n",
       "      <td>-0.670725</td>\n",
       "      <td>-1.705803</td>\n",
       "      <td>0.197479</td>\n",
       "      <td>0.712666</td>\n",
       "      <td>0.543947</td>\n",
       "      <td>-0.808352</td>\n",
       "      <td>0.597578</td>\n",
       "      <td>1.002218</td>\n",
       "      <td>0.672947</td>\n",
       "      <td>-0.127098</td>\n",
       "      <td>0.267151</td>\n",
       "      <td>0.043273</td>\n",
       "      <td>0.107183</td>\n",
       "      <td>-0.140980</td>\n",
       "      <td>-0.335164</td>\n",
       "      <td>0.579778</td>\n",
       "      <td>-0.036550</td>\n",
       "      <td>0.082515</td>\n",
       "      <td>0.042893</td>\n",
       "      <td>-0.023380</td>\n",
       "      <td>0.685178</td>\n",
       "      <td>0.103946</td>\n",
       "      <td>1.034348</td>\n",
       "      <td>0.848580</td>\n",
       "      <td>-0.032466</td>\n",
       "      <td>0.243719</td>\n",
       "      <td>0.145248</td>\n",
       "      <td>0.639411</td>\n",
       "      <td>-0.069999</td>\n",
       "      <td>0.188896</td>\n",
       "      <td>0.038189</td>\n",
       "      <td>0.929764</td>\n",
       "      <td>0.230777</td>\n",
       "      <td>-0.806072</td>\n",
       "      <td>-1.730868</td>\n",
       "      <td>0.341294</td>\n",
       "      <td>-0.314023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>-2.094868</td>\n",
       "      <td>-0.996378</td>\n",
       "      <td>-0.871700</td>\n",
       "      <td>-0.920379</td>\n",
       "      <td>0.193898</td>\n",
       "      <td>0.017109</td>\n",
       "      <td>-0.427439</td>\n",
       "      <td>1.126358</td>\n",
       "      <td>0.675815</td>\n",
       "      <td>-2.225879</td>\n",
       "      <td>0.588232</td>\n",
       "      <td>0.216891</td>\n",
       "      <td>-2.515471</td>\n",
       "      <td>0.455121</td>\n",
       "      <td>0.350450</td>\n",
       "      <td>0.358709</td>\n",
       "      <td>-0.540396</td>\n",
       "      <td>-0.344688</td>\n",
       "      <td>1.379858</td>\n",
       "      <td>-0.203664</td>\n",
       "      <td>0.293467</td>\n",
       "      <td>-0.451586</td>\n",
       "      <td>-0.202152</td>\n",
       "      <td>1.646159</td>\n",
       "      <td>0.669212</td>\n",
       "      <td>-0.195778</td>\n",
       "      <td>0.181725</td>\n",
       "      <td>-0.389433</td>\n",
       "      <td>1.292014</td>\n",
       "      <td>-0.533416</td>\n",
       "      <td>0.627960</td>\n",
       "      <td>0.697279</td>\n",
       "      <td>1.603717</td>\n",
       "      <td>0.399779</td>\n",
       "      <td>-0.955494</td>\n",
       "      <td>1.308291</td>\n",
       "      <td>-0.911537</td>\n",
       "      <td>-1.057855</td>\n",
       "      <td>-0.424855</td>\n",
       "      <td>-0.394519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293506</td>\n",
       "      <td>1.930882</td>\n",
       "      <td>0.431977</td>\n",
       "      <td>0.074280</td>\n",
       "      <td>-1.466985</td>\n",
       "      <td>-0.095761</td>\n",
       "      <td>0.247062</td>\n",
       "      <td>-0.461277</td>\n",
       "      <td>-1.476601</td>\n",
       "      <td>0.069654</td>\n",
       "      <td>0.824789</td>\n",
       "      <td>0.790260</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>-0.559995</td>\n",
       "      <td>0.318212</td>\n",
       "      <td>0.021219</td>\n",
       "      <td>-0.209655</td>\n",
       "      <td>1.030852</td>\n",
       "      <td>0.345151</td>\n",
       "      <td>0.674453</td>\n",
       "      <td>-0.189890</td>\n",
       "      <td>-1.271380</td>\n",
       "      <td>-1.128333</td>\n",
       "      <td>0.947941</td>\n",
       "      <td>1.096415</td>\n",
       "      <td>-0.117770</td>\n",
       "      <td>0.062011</td>\n",
       "      <td>0.671980</td>\n",
       "      <td>-0.123901</td>\n",
       "      <td>1.216709</td>\n",
       "      <td>-0.044727</td>\n",
       "      <td>-0.322464</td>\n",
       "      <td>-0.151085</td>\n",
       "      <td>0.025218</td>\n",
       "      <td>0.591481</td>\n",
       "      <td>0.469718</td>\n",
       "      <td>0.618129</td>\n",
       "      <td>0.416137</td>\n",
       "      <td>-0.563816</td>\n",
       "      <td>0.200399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2   ...        97        98        99\n",
       "one    -1.037543  0.256925 -0.456524  ...  0.098816  1.096606  1.468806\n",
       "people -1.624187  0.492345 -0.033345  ... -0.626744 -1.626490  1.004843\n",
       "like   -0.868234 -0.626363 -0.779085  ... -0.160890  0.909012 -1.139922\n",
       "know   -0.730419 -0.223894 -0.771613  ... -0.466870  0.795719 -0.620436\n",
       "going  -1.217457  0.998311 -0.800044  ... -0.315375 -0.844938  0.604256\n",
       "think  -0.698013 -0.350541  0.439105  ... -1.129160 -0.103626 -0.687824\n",
       "see     0.011837 -0.309356  0.814826  ... -0.984520  0.250803 -0.373136\n",
       "would  -0.314115  0.284673  1.002277  ... -0.993600 -0.702535 -1.063961\n",
       "really -2.165626 -1.054550 -0.125160  ... -1.730868  0.341294 -0.314023\n",
       "get    -2.094868 -0.996378 -0.871700  ...  0.416137 -0.563816  0.200399\n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(model.wv.vectors, index=model.wv.index_to_key).head(10)    # index_to_key : 인덱스 자리에 단어사전이 들어감 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3a04812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 임베딩 모델 저장\n",
    "model.wv.save_word2vec_format('ted_en_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 로드\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "load_model = KeyedVectors.load_word2vec_format('ted_en_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399877d",
   "metadata": {},
   "source": [
    "- 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce046d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8962153792381287),\n",
       " ('daughter', 0.7890374660491943),\n",
       " ('girl', 0.7854432463645935),\n",
       " ('lady', 0.7824974060058594),\n",
       " ('father', 0.7647601962089539),\n",
       " ('son', 0.7621195316314697),\n",
       " ('boy', 0.7620626091957092),\n",
       " ('brother', 0.7458978891372681),\n",
       " ('grandfather', 0.7357702255249023),\n",
       " ('sister', 0.7320916652679443)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')    # man에 대한 유사도 \n",
    "# model.wv.most_similar('abracadabra')    # 임베딩 벡터에 없는 단어 조회 시 key error가 발생 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5c51635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8906522989273071),\n",
       " ('girl', 0.8080182075500488),\n",
       " ('daughter', 0.7959010004997253),\n",
       " ('son', 0.7876567244529724),\n",
       " ('lady', 0.7868496179580688),\n",
       " ('grandfather', 0.7742390036582947),\n",
       " ('father', 0.7667587995529175),\n",
       " ('grandmother', 0.7577228546142578),\n",
       " ('sister', 0.752505898475647),\n",
       " ('boy', 0.7516880631446838)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.most_similar('man')  # Word2Vec.wv = KeyedVectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc755569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72041714"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man', 'husband')   # 두 단어간의 유사도 (학습 결과이기 때문에 조금씩 다를 수 있다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b00f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.15428603e+00,  1.66146472e-01,  8.74931633e-01,  1.71548569e+00,\n",
       "       -9.20389533e-01,  3.37695852e-02, -7.85597980e-01,  1.56282449e+00,\n",
       "       -6.69489682e-01, -1.04619658e+00, -4.47931513e-02,  5.90489388e-01,\n",
       "        8.13253641e-01,  7.34468579e-01,  7.59392917e-01, -4.78596717e-01,\n",
       "        1.00773227e+00, -1.41518131e-01, -1.13767982e+00, -6.63283706e-01,\n",
       "        6.64211631e-01,  1.28990185e+00, -8.04617479e-02,  7.56952912e-02,\n",
       "        1.23693466e-01, -4.77305770e-01, -7.38497496e-01, -1.13054931e+00,\n",
       "        2.51591444e-01,  1.29510498e+00, -1.27243352e+00, -1.70458579e+00,\n",
       "       -2.18518943e-01, -9.02804971e-01, -6.44786477e-01,  1.14883518e+00,\n",
       "       -5.48195302e-01, -5.57738185e-01,  5.35091460e-01,  2.99523026e-01,\n",
       "        8.87771070e-01,  4.44204926e-01,  7.82241642e-01,  4.12249297e-01,\n",
       "        2.11211872e+00,  5.84527075e-01, -4.46760476e-01,  8.21967185e-01,\n",
       "        6.83324039e-01, -3.67053509e-01,  9.39221501e-01, -3.72545302e-01,\n",
       "        1.04090892e-01, -2.18602791e-01,  4.38753478e-02,  6.41328931e-01,\n",
       "        2.53619760e-01,  8.13080549e-01, -2.92484432e-01, -8.11564386e-01,\n",
       "       -3.13563138e-01, -1.28671443e+00, -1.46359563e+00,  1.08001614e+00,\n",
       "       -7.78837860e-01,  7.22212076e-01, -2.33429387e-01,  8.03421199e-01,\n",
       "        3.32314223e-01,  1.40398169e+00,  1.81037257e-03, -5.24418056e-01,\n",
       "        2.30715856e-01, -9.96450245e-01,  2.66961783e-01,  7.77938008e-01,\n",
       "       -1.44458890e-01,  4.87149358e-01,  3.37918997e-01,  2.56902337e-01,\n",
       "       -8.14729393e-01,  9.86471951e-01, -1.08260727e+00,  1.09559751e+00,\n",
       "       -2.46357378e-02, -2.84124650e-02, -4.70447600e-01, -2.05185354e-01,\n",
       "        7.48233020e-01,  5.32297969e-01,  2.65290380e-01, -1.94068551e-01,\n",
       "        1.03181086e-01, -1.31022549e+00,  2.13040888e-01, -4.93603021e-01,\n",
       "        7.74219990e-01,  3.30263317e-01,  1.64786041e-01, -7.42034018e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb60b32",
   "metadata": {},
   "source": [
    "- 임베딩 시각화\n",
    "\n",
    "https://projector.tensorflow.org/\n",
    "\n",
    "- embedding vector(tensor) 파일 (.tsv)\n",
    "- metadata 파일 (.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48dfbd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 16:14:33,923 - word2vec2tensor - INFO - running c:\\Users\\USER\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input ted_en_w2v --output ted_en_w2v\n",
      "2025-04-07 16:14:33,924 - keyedvectors - INFO - loading projection weights from ted_en_w2v\n",
      "2025-04-07 16:14:34,969 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (21462, 100) matrix of type float32 from ted_en_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-04-07T16:14:34.810173', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'load_word2vec_format'}\n",
      "2025-04-07 16:14:35,687 - word2vec2tensor - INFO - 2D tensor file saved to ted_en_w2v_tensor.tsv\n",
      "2025-04-07 16:14:35,687 - word2vec2tensor - INFO - Tensor metadata file saved to ted_en_w2v_metadata.tsv\n",
      "2025-04-07 16:14:35,687 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input ted_en_w2v --output ted_en_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c2abeb",
   "metadata": {},
   "source": [
    "##### 한국어 Word Embedding\n",
    "- NSMC (Naver Sentiment Movie Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10c51c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import urllib.request\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f79f759e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('naver_movie_ratings.txt', <http.client.HTTPMessage at 0x1cd279bf680>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\",\n",
    "    filename=\"naver_movie_ratings.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40747a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 생성\n",
    "ratings_df = pd.read_csv('naver_movie_ratings.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5ebbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    8\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결측치 확인 및 처리(제거)\n",
    "display(ratings_df.isnull().sum())\n",
    "\n",
    "ratings_df = ratings_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "660a1264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    많은 생각을 할 수 있는 영화~ 시간여행류의 스토리를 좋아하는 사람이라면 빠트릴 수...\n",
       "201    고소한 19 정말 재미있게 잘 보고 있습니다^^ 방송만 보면 털털하고 인간적이신 것...\n",
       "202                                                  가연세\n",
       "203                         goodgoodgoodgoodgoodgoodgood\n",
       "204                                           이물감. 시 같았다\n",
       "                             ...                        \n",
       "295                                   박력넘치는 스턴트 액션 평작이다!\n",
       "296                                      엄청 재미있다 명작이다 ~~\n",
       "297    나는 하정우랑 개그코드가 맞나보다 엄청 재밌게봤네요 특히 단발의사샘 장면에서 계속 ...\n",
       "298                                                적당 ㅎㅎ\n",
       "299                                    배경이 이쁘고 캐릭터도 귀엽네~\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df['document'][200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07af4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글이 아닌 데이터 제거\n",
    "ratings_df['document'] = ratings_df['document'].replace(r'[^0-9가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af696750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199992/199992 [07:24<00:00, 449.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "from tqdm import tqdm   # 진행도 시각화\n",
    "\n",
    "okt = Okt()\n",
    "ko_stopwords = ['은', '는', '이', '가', '을', '를', '와', '과', '들', '도', '부터', '까지', '에', '나', '너', '그', '걔', '얘']\n",
    "\n",
    "preprocessed_data = []\n",
    "\n",
    "for sentence in tqdm(ratings_df['document']):\n",
    "    tokens = okt.morphs(sentence, stem=True)\n",
    "    tokens = [token for token in tokens if token not in ko_stopwords]\n",
    "    preprocessed_data.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54bf63cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16841, 100)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=preprocessed_data,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=0    # CBOW\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a09edce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영화관', 0.942000150680542),\n",
       " ('케이블', 0.7884935140609741),\n",
       " ('틀어주다', 0.766829252243042),\n",
       " ('학교', 0.7575247883796692),\n",
       " ('티비', 0.7262733578681946),\n",
       " ('대학로', 0.7220457792282104),\n",
       " ('방금', 0.7055728435516357),\n",
       " ('영화제', 0.7023119926452637),\n",
       " ('시사회', 0.6865780353546143),\n",
       " ('개봉관', 0.6836691498756409)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('극장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4493925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87271345"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('김혜수', '전도연')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df5e5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.wv.save_word2vec_format('naver_movie_ratings_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e463111b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:11:10,096 - word2vec2tensor - INFO - running c:\\Users\\USER\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input naver_movie_ratings_w2v --output naver_movie_ratings_w2v\n",
      "2025-04-07 17:11:10,096 - keyedvectors - INFO - loading projection weights from naver_movie_ratings_w2v\n",
      "2025-04-07 17:11:11,068 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (16841, 100) matrix of type float32 from naver_movie_ratings_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-04-07T17:11:10.818605', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'load_word2vec_format'}\n",
      "2025-04-07 17:11:11,620 - word2vec2tensor - INFO - 2D tensor file saved to naver_movie_ratings_w2v_tensor.tsv\n",
      "2025-04-07 17:11:11,620 - word2vec2tensor - INFO - Tensor metadata file saved to naver_movie_ratings_w2v_metadata.tsv\n",
      "2025-04-07 17:11:11,620 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input naver_movie_ratings_w2v --output naver_movie_ratings_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c42522",
   "metadata": {},
   "source": [
    "- 사전 훈련된 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66194b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c\n",
      "From (redirected): https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c&confirm=t&uuid=bb8a4442-822e-4514-b1e2-b90551199429\n",
      "To: c:\\encore_skn11\\07_nlp\\03_word_embedding\\GoogleNews_vecs.bins.gz\n",
      "100%|██████████| 1.65G/1.65G [05:20<00:00, 5.14MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GoogleNews_vecs.bins.gz'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c'\n",
    "output = \"GoogleNews_vecs.bins.gz\"\n",
    "\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86ca7fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv = KeyedVectors.load_word2vec_format('GoogleNews_vecs.bins.gz', binary=True)\n",
    "google_news_wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f547fe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22942671"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.similarity('king','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "631826c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138044834136963),\n",
       " ('queen', 0.6510957479476929),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159994602203369),\n",
       " ('sultan', 0.5864822864532471),\n",
       " ('ruler', 0.5797566175460815),\n",
       " ('princes', 0.5646552443504333),\n",
       " ('Prince_Paras', 0.5432944297790527),\n",
       " ('throne', 0.5422106385231018)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a8bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24791394"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.n_similarity(['king', 'queen'], ['man', 'woman'])    # 두 리스트간의 평균 유사도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cdc500f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138044834136963),\n",
       " ('queen', 0.6510957479476929),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159994602203369)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.most_similar('king', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f3026c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138044834136963),\n",
       " ('queen', 0.6510957479476929),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159994602203369)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.similar_by_word('king', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de6cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.has_index_for('ㅋㅋㅋㅋㅋㅋ')    # 모델이 학습한 vocabulary에 존재하는지 아닌지 체크할 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1488f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
