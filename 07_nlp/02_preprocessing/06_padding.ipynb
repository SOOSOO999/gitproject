{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "- 자연어 처리에서 각 문장(문서)의 길이는 다를 수 있음\n",
    "- 그러나 언어모델은 고정된 길이의 데이터를 효율적으로 처리함함\n",
    "    - -> 모든 문장의 길이를 동일하게 맞춰주는 작업이 필요함 == 패딩\n",
    "\n",
    "**패딩 이점**\n",
    "1. 일관된 입력 형식\n",
    "2. 병렬 연산 최적화\n",
    "3. 유연한 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델(특히 RNN, LSTM, Transformer 등)은 입력 시퀀스의 길이가 동일해야 하기 때문에,\n",
    "# 짧은 문장은 긴 문장에 맞춰 일정한 길이로 늘려줘야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'],\n",
    "                          ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
    "                          ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
    "                          ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
    "                          ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "class TokenizerForPadding:\n",
    "    def __init__(self, num_words=None, oov_token='<OOV>'):  # num_words : 단어사전의 내용의 개수 \n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        self.word_counts = Counter()    # 전체 단어 빈도를 담는 Counter / 리스트 같은 반복 가능한(iterable) 객체에서 원소의 개수를 자동으로 세어주는 딕셔너리 형태의 클래스\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        # 빈도수 세기 \n",
    "        for sentence in texts:\n",
    "            self.word_counts.update(word for word in sentence if word)\n",
    "\n",
    "        # 빈도수 기반 vocabulary 생성 (num_words 만큼만) \n",
    "        vocab = [self.oov_token] + [word for word, _ in self.word_counts.most_common(self.num_words -2 if self.num_words else None)]\n",
    "        # most_common(): 가장 많이 등장한 항목들을 (원소, 개수) 쌍의 리스트로 반환 \n",
    "        # self.num_words가 존재하면 -2 (0와 OOV 두개를 뺀 값) / most_common : (self.num_words -2)개의 숫자만큼 빈도수가 상위인 것을 가져온다\n",
    "        self.word_index = {word: i+1 for i, word in enumerate(vocab)}\n",
    "        self.index_word = {i+1: word for word, i in self.word_index.items()}\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        return [[self.word_index.get(word, self.word_index[self.oov_token]) for word in sentence] for sentence in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequneces, maxlen=None, padding='pre', truncating='pre', value=0):    # 'pre' : 패딩을 앞에 추가 / value=0 : 제로패딩 (빈공간을 0으로 채워주겠다)\n",
    "    if maxlen is None:  # 일괄적으로 정해줄 길이\n",
    "        maxlen = max(len(seq) for seq in sequneces)     # 가장 긴 문장에 맞춰서 설정할 수 있게. \n",
    "\n",
    "    padded_sequences = []\n",
    "    for seq in sequneces:\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'pre':\n",
    "                seq = seq[-maxlen:]\n",
    "            else:   # post\n",
    "                seq = seq[:maxlen]\n",
    "        else:\n",
    "            pad_length = maxlen - len(seq)\n",
    "            if padding == 'pre':\n",
    "                seq = [value] * pad_length + seq\n",
    "            else:   # post \n",
    "                seq = seq + [value] * pad_length\n",
    "        padded_sequences.append(seq)\n",
    "    \n",
    "    return torch.tensor(padded_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6],\n",
       " [2, 9, 6],\n",
       " [2, 4, 6],\n",
       " [10, 3],\n",
       " [3, 5, 4, 3],\n",
       " [4, 3],\n",
       " [2, 5, 7],\n",
       " [2, 5, 7],\n",
       " [2, 5, 3],\n",
       " [8, 8, 4, 3, 11, 2, 12],\n",
       " [2, 13, 4, 14]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerForPadding(num_words=15)\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequneces = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "sequneces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  2,  6],\n",
       "        [ 0,  0,  0,  0,  2,  9,  6],\n",
       "        [ 0,  0,  0,  0,  2,  4,  6],\n",
       "        [ 0,  0,  0,  0,  0, 10,  3],\n",
       "        [ 0,  0,  0,  3,  5,  4,  3],\n",
       "        [ 0,  0,  0,  0,  0,  4,  3],\n",
       "        [ 0,  0,  0,  0,  2,  5,  7],\n",
       "        [ 0,  0,  0,  0,  2,  5,  7],\n",
       "        [ 0,  0,  0,  0,  2,  5,  3],\n",
       "        [ 8,  8,  4,  3, 11,  2, 12],\n",
       "        [ 0,  0,  0,  2, 13,  4, 14]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequneces)   # 데이터의 길이를 맞춰주는게 패딩. / 패딩을 앞으로 붙임 \n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  6,  0,  0,  0],\n",
       "        [ 2,  9,  6,  0,  0],\n",
       "        [ 2,  4,  6,  0,  0],\n",
       "        [10,  3,  0,  0,  0],\n",
       "        [ 3,  5,  4,  3,  0],\n",
       "        [ 4,  3,  0,  0,  0],\n",
       "        [ 2,  5,  7,  0,  0],\n",
       "        [ 2,  5,  7,  0,  0],\n",
       "        [ 2,  5,  3,  0,  0],\n",
       "        [ 8,  8,  4,  3, 11],\n",
       "        [ 2, 13,  4, 14,  0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequneces, padding='post', maxlen=5, truncating='post')   # 패딩을 뒤로 붙임 \n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras Tokenizer 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequneces = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "sequneces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0],\n",
       "       [ 1,  8,  5],\n",
       "       [ 1,  3,  5],\n",
       "       [ 9,  2,  0],\n",
       "       [ 2,  4,  3],\n",
       "       [ 3,  2,  0],\n",
       "       [ 1,  4,  6],\n",
       "       [ 1,  4,  6],\n",
       "       [ 1,  4,  2],\n",
       "       [ 7,  7,  3],\n",
       "       [ 1, 12,  3]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(sequneces, padding='post', maxlen=3, truncating='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 어린왕자 데이터 샘플 패딩처리 (실습) **(월요일까지)**\n",
    "\n",
    "1. 텍스트 전처리 (토큰화/불용어처리/정제/정규화)\n",
    "2. 정수 인코딩 Tokenizer (tensorflow.keras)\n",
    "3. 패딩 처리 pad_sequences (tensorflow.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"The Little Prince, written by Antoine de Saint-Exupéry, is a poetic tale about a young prince who travels from his home planet to Earth. The story begins with a pilot stranded in the Sahara Desert after his plane crashes. While trying to fix his plane, he meets a mysterious young boy, the Little Prince.\n",
    "\n",
    "The Little Prince comes from a small asteroid called B-612, where he lives alone with a rose that he loves deeply. He recounts his journey to the pilot, describing his visits to several other planets. Each planet is inhabited by a different character, such as a king, a vain man, a drunkard, a businessman, a geographer, and a fox. Through these encounters, the Prince learns valuable lessons about love, responsibility, and the nature of adult behavior.\n",
    "\n",
    "On Earth, the Little Prince meets various creatures, including a fox, who teaches him about relationships and the importance of taming, which means building ties with others. The fox's famous line, \"You become responsible, forever, for what you have tamed,\" resonates with the Prince's feelings for his rose.\n",
    "\n",
    "Ultimately, the Little Prince realizes that the essence of life is often invisible and can only be seen with the heart. After sharing his wisdom with the pilot, he prepares to return to his asteroid and his beloved rose. The story concludes with the pilot reflecting on the lessons learned from the Little Prince and the enduring impact of their friendship.\n",
    "\n",
    "The narrative is a beautifully simple yet profound exploration of love, loss, and the importance of seeing beyond the surface of things.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
