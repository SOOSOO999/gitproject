{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebf7d26",
   "metadata": {},
   "source": [
    "# HuggingFace Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets\n",
    "# !pip install -q sentencepiece\n",
    "# !pip install -q kobert-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba0381",
   "metadata": {},
   "source": [
    "# NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7816f2",
   "metadata": {},
   "source": [
    "ì£¼ì–´ì§„ taskì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ `pipeline`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.\n",
    "\n",
    "ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì€ ì£¼ìš” taskë¥¼ ì§€ì›í•œë‹¤:\n",
    "\n",
    "- **ê¸°ê³„ ë²ˆì—­(Translation)**: í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•œë‹¤.  \n",
    "- **ê°ì • ë¶„ì„(Text Classification)**: í…ìŠ¤íŠ¸ê°€ ê¸ì •ì ì¸ì§€ ë¶€ì •ì ì¸ì§€ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.  \n",
    "- **í…ìŠ¤íŠ¸ ìƒì„±(Text Generation)**: í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ëª¨ë¸ì´ í›„ì† í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.  \n",
    "- **ì´ë¦„ ê°œì²´ ì¸ì‹(NER)**: ì…ë ¥ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ê°€ ì–´ë–¤ ê°œì²´(ì˜ˆ: ì‚¬ëŒ, ì¥ì†Œ ë“±)ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ì§€ ì‹ë³„í•  ìˆ˜ ìˆë‹¤.  \n",
    "- **ì§ˆë¬¸ ë‹µë³€(Question Answering)**: ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ëª¨ë¸ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì ì ˆí•œ ë‹µë³€ì„ ì¶”ì¶œí•œë‹¤.  \n",
    "- **ë§ˆìŠ¤í‚¹ëœ í…ìŠ¤íŠ¸ ì±„ìš°ê¸°(Fill-Mask)**: ë§ˆìŠ¤í‚¹ëœ ë‹¨ì–´ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸(`[MASK]`ë¡œ ëŒ€ì²´ë¨)ë¥¼ ì…ë ¥í•˜ë©´ ê³µë°±ì„ ì±„ìš¸ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•œë‹¤.  \n",
    "- **ìš”ì•½(Summarization)**: ê¸´ í…ìŠ¤íŠ¸ì˜ ìš”ì•½ì„ ìƒì„±í•œë‹¤.  \n",
    "- **íŠ¹ì§• ì¶”ì¶œ(Feature Extraction)**: í…ìŠ¤íŠ¸ì˜ í…ì„œ í‘œí˜„ì„ ë°˜í™˜í•˜ì—¬ íŠ¹ì„±ì„ ì¶”ì¶œí•œë‹¤.  \n",
    "- **Zero-Shot ë¶„ë¥˜(Zero-Shot Classification)**: ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì— ëŒ€í•´ ì‚¬ì „ ì •ì˜ëœ ë ˆì´ë¸”ì— ë§ëŠ” ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•œë‹¤.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ffda3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a99692",
   "metadata": {},
   "source": [
    "### ê¸°ê³„ë²ˆì—­   \n",
    "https://huggingface.co/Helsinki-NLP/opus-mt-ko-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b4721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dheum/.conda/envs/colab/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.TranslationPipeline at 0x7f3c77493ed0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-ko-en')\n",
    "translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9980ee7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"It's Tuesday, so I'm gonna have to eat something good!\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('ì˜¤ëŠ˜ì€ í™”ìš”ì¼ì´ë‹ˆê¹Œ ë§›ìˆëŠ”ê±¸ ë¨¹ì–´ì•¼ê² ì–´ìš”!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6aa703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Today, we're going to learn an outline from LLM.\"},\n",
       " {'translation_text': \"Now that you've studied the Transpomer, you've crossed a large mountain.\"},\n",
       " {'translation_text': \"You've grown up!\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator([\n",
    "    'ì˜¤ëŠ˜ì€ LLMì˜ ê°œìš”ë¥¼ ë°°ì›Œë³¼ ê±°ì—ìš”',\n",
    "    'íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ë‹¤ ê³µë¶€í–ˆìœ¼ë‹ˆ ì—¬ëŸ¬ë¶„ì€ í° ì‚°ì„ ë„˜ì—ˆë‹µë‹ˆë‹¤~',\n",
    "    'ì•„ì£¼ ë©‹ìˆì–´! ì•„ì£¼ ì„±ì¥í–ˆì–´~!!!'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fadb7b4",
   "metadata": {},
   "source": [
    "### ê°ì„± ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9004111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97146beb691433d834058f22a88f1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846a6f59b14345efb2c394ca300062e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a239f0c918e42819f7735f2aaf4f5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b18ffcdc863473894f3b9e9a121a2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "sentiment_clf = pipeline('sentiment-analysis')  # ëª¨ë¸ ì§€ì • ì•ˆí•˜ë©´ default ëª¨ë¸ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf619bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9992774128913879}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_clf(\"I'm very hungry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c535fa9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9993190765380859},\n",
       " {'label': 'NEGATIVE', 'score': 0.9932923316955566},\n",
       " {'label': 'POSITIVE', 'score': 0.9996789693832397}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_clf([\n",
    "    \"My exam was failed\",\n",
    "    \"Spring weather is so weird\",\n",
    "    \"Rabbit teacher is very nice\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1fbbe5",
   "metadata": {},
   "source": [
    "### í•œêµ­ì–´ ê°ì„± ë¶„ì„   \n",
    "https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d29ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4274ee9e00e42e58666ef74fc244741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  22%|##2       | 157M/712M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bfa0103c70461e8a48dd2335e3b73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b2afc7dcff4b89bdf3757ec3789be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7433bbe2b3e2474db62297b51e936bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/711M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b389a7a2f04949e9ab1b549b8576298e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e587a287d25a4b8b8a2ba7240a9c7206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "ko_sentiment_clf = pipeline(\"sentiment-analysis\", model=\"sangrimlee/bert-base-multilingual-cased-nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6394c521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9273295998573303},\n",
       " {'label': 'negative', 'score': 0.7013086080551147},\n",
       " {'label': 'positive', 'score': 0.5110437870025635},\n",
       " {'label': 'negative', 'score': 0.6181637048721313}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_sentiment_clf([\n",
    "    \"ì–´ì œ ë°°ìš´ê²Œ ì–´ë ¤ì›Œì„œ, ë§¥ì£¼ë¥¼ í•œ ì” í–ˆì–´.\",\n",
    "    \"ê³µë¶€ëŠ” ì–´ë µì§€ë§Œ ë§¥ì£¼ëŠ” ì‹œì›í•˜ë”ë¼\",\n",
    "    \"ê·¸ë˜ë„ ë‚˜ëŠ” ì˜¤ëŠ˜ë„ ê³µë¶€í• ê±°ì•¼\",\n",
    "    \"ë§¥ì£¼ëŠ” ë§›ì—†ì§€ ì•Šì•„!!!\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63ab21",
   "metadata": {},
   "source": [
    "### Zero Shot Classification\n",
    "- shot == ì˜ˆì‹œ\n",
    "- ì˜ˆì‹œ ì—†ì´ (í•™ìŠµ ì—†ì´) ì¶”ë¡  ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ccee49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48e186e6afa4c2890748da1836cdee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e304556852a4a48ba58ee5dc88e4675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0dd02d68184455b2d6118f67050a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371e7e93666341809f1f1cfb902104a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1382e85b184449ab7ff2621a219cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f114893c0045e189ca0e054e98d534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "zero_shot_clf = pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168dbf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9192402958869934, 0.060778748244047165, 0.01998094469308853]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_clf(\n",
    "    \"This is a course about the transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40e7ac35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"It has a poison and it's very dangerous\",\n",
       " 'labels': ['snake', 'squirrel', 'rabbit'],\n",
       " 'scores': [0.6727704405784607, 0.17300079762935638, 0.15422874689102173]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_clf(\n",
    "    \"It has a poison and it's very dangerous\",\n",
    "    candidate_labels=[\"rabbit\", \"snake\", \"squirrel\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba33330",
   "metadata": {},
   "source": [
    "### í•œêµ­ì–´ Zero Shot   \n",
    "https://huggingface.co/joeddav/xlm-roberta-large-xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12f0aeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a66ab4f59a4e01a0863025d175d4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/734 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de20ee5e5c9c47f58c8b477d9d604d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735bdb0c83444305917a854732e4c72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db18ff176ef443cc9e8691c7e1f22c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30f6be225f0438cbf99bdf3905b1feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "ko_zero_shot_clf = pipeline('zero-shot-classification', model='joeddav/xlm-roberta-large-xnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1a846c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '2025ë…„ ì–´ë–¤ ìš´ë™ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?!',\n",
       " 'labels': ['ê±´ê°•', 'ì •ì¹˜', 'ê²½ì œ'],\n",
       " 'scores': [0.9117885231971741, 0.05933362618088722, 0.02887781709432602]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '2025ë…„ ì–´ë–¤ ìš´ë™ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?!'\n",
    "candidate_labels=['ì •ì¹˜', 'ê²½ì œ', 'ê±´ê°•']\n",
    "hypothesis_template = 'ì´ í…ìŠ¤íŠ¸ëŠ” {}ì— ê´€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.'\n",
    "\n",
    "ko_zero_shot_clf(\n",
    "    sentence,\n",
    "    candidate_labels=candidate_labels,\n",
    "    hypothesis_template=hypothesis_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03240afc",
   "metadata": {},
   "source": [
    "### í…ìŠ¤íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fddcf6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6872b24729f04d3e89c72114c5db2a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115bdd720a8e47e2bbd0e08e574867c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10abcf8dc0441f49e3c88302e8cfc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b157795169c1431682b89f391f602eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa53ad86de54ce68ffd986c2f5a661d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb042f464f3c41e5b9ee95e8706270df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline('text-generation', model='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5d2f018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to change how you view and to learn a lot about using it on your own day-to-day'},\n",
       " {'generated_text': 'In this course, we will teach you how to think of the ways in which humans are capable of thinking that they are capable of thinking like all humans'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\n",
    "    'In this course, we will teach you how to',\n",
    "    max_length=30,\n",
    "    num_return_sequences=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572da72c",
   "metadata": {},
   "source": [
    "### í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„±   \n",
    "https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d2e0b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ad6e217d024e72a5c986c5ff26b689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ea304a07074103af5aca412eb1bc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b163616ef24ad4907303153748b1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086a13206c4d4455928a592e009e4fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a28789fdbba4f3bbd7e562fac442a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77d26ecd26b48e5a4156be284a7226e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "ko_text_generator = pipeline('text-generation', model='skt/ko-gpt-trinity-1.2B-v0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13a38d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'ì˜¤ëŠ˜ ì ì‹¬ì—ëŠ” ë­ ë¨¹ì§€\\n ë‹µë³€:í•œêµ­ì¸ì€ ë°¥ì‹¬ì´ì£ . ë“ ë“ í•˜ê²Œ ì±™ê²¨ë“œì„¸ìš”.'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_text_generator(\"ì˜¤ëŠ˜ ì ì‹¬ì—ëŠ”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "698e4dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'í† ë¼ëŠ” ê·€ê°€ 2ê°œë‹¤ (... ). ê·¸ë¦¬ê³  ì´ ë‘ ê°œì˜ ê·€ê°€ ê°ê° ë‹¤ë¥¸ ë™ë¬¼ë“¤ì˜ ê·€ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì´ ë‘ ê°œì˜ ê·€ê°€'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_text_generator(\"í† ë¼ëŠ” ê·€ê°€ 2ê°œë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0b2c5",
   "metadata": {},
   "source": [
    "### Fill Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1ed7205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355aeb81db1b4a86b7282c52b9700b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4cb3410e574d88b499f9eb945bdb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41357743b4743c5bf1181521b307af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717130c8d2bb4e4681e2961c0955c49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13773bcf46b346e584023b044e5c56af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3571ca11646c4323a0d5a9bcae51f21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34c3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.08884059637784958,\n",
       "  'token': 265,\n",
       "  'token_str': ' business',\n",
       "  'sequence': 'This course will teach you all about business model.'},\n",
       " {'score': 0.07198566198348999,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical model.'},\n",
       " {'score': 0.04102081060409546,\n",
       "  'token': 5,\n",
       "  'token_str': ' the',\n",
       "  'sequence': 'This course will teach you all about the model.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\n",
    "    \"This course will teach you all about <mask> model.\",   # <mask> ì•ˆì— ë“¤ì–´ê°ˆë§Œí•œ ë‹¨ì–´ë¥¼ ì±„ì›Œì¤Œ\n",
    "    top_k=3                                                 # scoreê°€ ë†’ì€ê²ƒìœ¼ë¡œ ëª‡ ê°œë¥¼ ë³´ì—¬ì¤„ ì§€\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9a812",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6ddfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5902f04f9448468aa888e1a2b21915e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6813b53ff5421aab6aadd432a785b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dc83f83ac54d0f9123911290c5be15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67660703414c488ca9e5eaec20a7ea7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68e09364baa4afba4137e3c56e91304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32f9967bbb049cfaad1d51287eb836b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/dheum/.conda/envs/colab/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\n",
    "    'ner',\n",
    "    model='dslim/bert-base-NER',\n",
    "    aggregation_strategy=\"simple\"   # í† í° ë‹¨ìœ„ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì—”í‹°í‹° ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” ex) \"Bar\", \"##ack\", \"Obama\" â†’ \"Barack Obama\" â†’ PER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02227e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9943137,\n",
       "  'word': 'Rabbit',\n",
       "  'start': 11,\n",
       "  'end': 17},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9896114,\n",
       "  'word': 'Playdata',\n",
       "  'start': 32,\n",
       "  'end': 40},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99942976,\n",
       "  'word': 'Seoul',\n",
       "  'start': 44,\n",
       "  'end': 49}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"My name is Rabbit and I work at Playdata in Seoul.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf2c1c",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b11e3425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b1d51467784bcdbcdc54568cb72f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d340877be7ef4183b0e1cd58833402fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0092db39874bf7986c4723879ab740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c843f4807194332a504888f462e2352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bd00f0eaa341958b1572844e83cddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "qna = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75dc6259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8229941129684448, 'start': 32, 'end': 40, 'answer': 'Playdata'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My Name is Rabbit and I work at Playdata in Seoul.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5febd",
   "metadata": {},
   "source": [
    "### í•œêµ­ì–´ Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c70a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "ko_qna = pipeline('question-answering', model='klue/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1a3272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 3.902253956766799e-05,\n",
       " 'start': 512,\n",
       " 'end': 536,\n",
       " 'answer': 'ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ì„œìš¸ì²­ìš´ì´ˆë“±í•™êµ ì—°ê·¹ë¶€ì—ì„œ'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_qna(\n",
    "    question=\"ì•„ë¥´ë°”ì´íŠ¸ëŠ” ì–´ë””ì„œ í–ˆë‚˜ìš”?\",\n",
    "    context='''ì–´ë ¸ì„ ë•Œë¶€í„° ê¿ˆì´ ë°°ìš°ì¸ ê±´ ì•„ë‹ˆì—ˆìœ¼ë©°, ì‚¬ë²”ëŒ€ ì§„í•™ì„ ëª©í‘œë¡œ ì¬ìˆ˜í•´ì„œ ê³µë¶€í•˜ë˜ ì¤‘, ë„ˆë¬´ ì¡¸ë¦° ë‚˜ë¨¸ì§€ ì„œì„œ ê³µë¶€í•˜ë ¤ë‹¤ê°€ ì„  ì±„ë¡œ ë‘ ì‹œê°„ì´ë‚˜ ì¡´ ê±¸ ê¹¨ë‹«ì ìì‹ ì€ ê³µë¶€ì™€ ì•ˆ ë§ëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•´ ì—°ê¸°ë¡œ ì§„ë¡œë¥¼ ë°”ê¾¸ê¸°ë¡œ ê²°ì‹¬í–ˆë‹¤ê³  í–ˆë‹¤. ì´í›„ ì—°ê·¹ì˜í™”ê³¼ì— ì§„í•™í•˜ì˜€ë‹¤.\n",
    "ëŒ€í•™ ìƒí™œë„ í‰ë²”í–ˆë‹¤ê³  í•œë‹¤. ì´ëŸ°ì €ëŸ° ë‹¨í¸ ì˜í™”ì— ì¶œì—°í•˜ì˜€ê³ , ë³‘ì—­ì€ ìˆ˜ì›ì‹œ ì˜í†µêµ¬ì²­ ê°€ì •ë³µì§€ê³¼ì—ì„œ ê³µìµê·¼ë¬´ìš”ì›ìœ¼ë¡œ ë§ˆì³¤ìœ¼ë©°, ê·¸ ì¦ˆìŒ 5ë…„ê°„ ì¹˜ì•„êµì •ì„ í–ˆë‹¤ê³  í•œë‹¤. ê·¸ì˜ ë°ë·”ì‘ì¸ 2014ë…„ í´ë˜ì§€ì½°ì´ì˜ 'ë‚´ê²Œ ëŒì•„ì™€' ë®¤ì§ë¹„ë””ì˜¤ë¶€í„° 2015ë…„ 3ì›” 12ì¼ì— ê°œë´‰í•œ ì˜í™” ã€Šì†Œì…œí¬ë¹„ì•„ã€‹ê¹Œì§€ëŠ” êµì •ê¸°ë¥¼ í•œ ê·¸ì˜ ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤. ëŒ€í•™ ì‹œì ˆë¶€í„° 2015ë…„ ìƒë°˜ê¸°ê¹Œì§€ëŠ” ì•ˆ í•´ë³¸ ì•Œë°”ê°€ ì—†ì„ ì •ë„ë¼ëŠ”ë° í”¼ì ë°°ë‹¬, ë§‰ë…¸ë™, ë§ˆíŠ¸ ìƒí•˜ì°¨, ì¼€ì´í„°ë§, ê³ ê¹ƒì§‘ ì„œë¹™, ìŒ€êµ­ìˆ˜ ê°€ê²Œ ì„œë¹™, í¸ì˜ì  ì•„ë¥´ë°”ì´íŠ¸, ì•„ì´ìŠ¤í¬ë¦¼ ì „ë¬¸ì , ëŒì”ì¹˜ ë° ê²°í˜¼ì‹ ì‚¬íšŒ, ì•„ì´ëŒ ì½˜ì„œíŠ¸ ê³µì‹ êµ¿ì¦ˆ íŒë§¤ ë“± ê·¸ ì¢…ë¥˜ë„ ë§¤ìš° ë‹¤ì–‘í•˜ë‹¤.[7] ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ì„¸ëª…ì´ˆë“±í•™êµì™€ ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ì„œìš¸ì²­ìš´ì´ˆë“±í•™êµ ì—°ê·¹ë¶€ì—ì„œ ë°©ê³¼í›„ êµì‚¬ë¡œ ì—°ê·¹ê³¼ ë®¤ì§€ì»¬ì„ ì§€ë„í•˜ê¸°ë„ í–ˆë‹¤.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece66193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on QuestionAnsweringPipeline in module transformers.pipelines.question_answering object:\n",
      "\n",
      "class QuestionAnsweringPipeline(transformers.pipelines.base.ChunkPipeline)\n",
      " |  QuestionAnsweringPipeline(model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Optional[transformers.modelcard.ModelCard] = None, framework: Optional[str] = None, task: str = '', **kwargs)\n",
      " |  \n",
      " |  Question Answering pipeline using any `ModelForQuestionAnswering`. See the [question answering\n",
      " |  examples](../task_summary#question-answering) for more information.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |  \n",
      " |  >>> oracle = pipeline(model=\"deepset/roberta-base-squad2\")\n",
      " |  >>> oracle(question=\"Where do I live?\", context=\"My name is Wolfgang and I live in Berlin\")\n",
      " |  {'score': 0.9191, 'start': 34, 'end': 40, 'answer': 'Berlin'}\n",
      " |  ```\n",
      " |  \n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      " |  \n",
      " |  This question answering pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"question-answering\"`.\n",
      " |  \n",
      " |  The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the\n",
      " |  up-to-date list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=question-answering).\n",
      " |  \n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      tokenizer ([`PreTrainedTokenizer`]):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`PreTrainedTokenizer`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |      torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |          the raw output data e.g. text.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      QuestionAnsweringPipeline\n",
      " |      transformers.pipelines.base.ChunkPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Answer the question(s) given as inputs by using the context(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          question (`str` or `List[str]`):\n",
      " |              One or several question(s) (must be used in conjunction with the `context` argument).\n",
      " |          context (`str` or `List[str]`):\n",
      " |              One or several context(s) associated with the question(s) (must be used in conjunction with the\n",
      " |              `question` argument).\n",
      " |          top_k (`int`, *optional*, defaults to 1):\n",
      " |              The number of answers to return (will be chosen by order of likelihood). Note that we return less than\n",
      " |              top_k answers if there are not enough options available within the context.\n",
      " |          doc_stride (`int`, *optional*, defaults to 128):\n",
      " |              If the context is too long to fit with the question for the model, it will be split in several chunks\n",
      " |              with some overlap. This argument controls the size of that overlap.\n",
      " |          max_answer_len (`int`, *optional*, defaults to 15):\n",
      " |              The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\n",
      " |          max_seq_len (`int`, *optional*, defaults to 384):\n",
      " |              The maximum length of the total sentence (context + question) in tokens of each chunk passed to the\n",
      " |              model. The context will be split in several chunks (using `doc_stride` as overlap) if needed.\n",
      " |          max_question_len (`int`, *optional*, defaults to 64):\n",
      " |              The maximum length of the question after tokenization. It will be truncated if needed.\n",
      " |          handle_impossible_answer (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not we accept impossible as an answer.\n",
      " |          align_to_words (`bool`, *optional*, defaults to `True`):\n",
      " |              Attempts to align the answer to real words. Improves quality on space separated languages. Might hurt on\n",
      " |              non-space-separated languages (like Japanese or Chinese)\n",
      " |      \n",
      " |      Return:\n",
      " |          A `dict` or a list of `dict`: Each result comes as a dictionary with the following keys:\n",
      " |      \n",
      " |          - **score** (`float`) -- The probability associated to the answer.\n",
      " |          - **start** (`int`) -- The character start index of the answer (in the tokenized version of the input).\n",
      " |          - **end** (`int`) -- The character end index of the answer (in the tokenized version of the input).\n",
      " |          - **answer** (`str`) -- The answer to the question.\n",
      " |  \n",
      " |  __init__(self, model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Optional[transformers.modelcard.ModelCard] = None, framework: Optional[str] = None, task: str = '', **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_indices(self, enc: 'tokenizers.Encoding', s: int, e: int, sequence_index: int, align_to_words: bool) -> Tuple[int, int]\n",
      " |  \n",
      " |  postprocess(self, model_outputs, top_k=1, handle_impossible_answer=False, max_answer_len=15, align_to_words=True)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |  \n",
      " |  preprocess(self, example, padding='do_not_pad', doc_stride=None, max_question_len=64, max_seq_len=None)\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |  \n",
      " |  span_to_answer(self, text: str, start: int, end: int) -> Dict[str, Union[int, str]]\n",
      " |      When decoding from token probabilities, this method maps token indexes to actual word in the initial context.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`): The actual context to extract the answer from.\n",
      " |          start (`int`): The answer starting token index.\n",
      " |          end (`int`): The answer end token index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dictionary like `{'answer': str, 'start': int, 'end': int}`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  create_sample(question: Union[str, List[str]], context: Union[str, List[str]]) -> Union[transformers.data.processors.squad.SquadExample, List[transformers.data.processors.squad.SquadExample]]\n",
      " |      QuestionAnsweringPipeline leverages the [`SquadExample`] internally. This helper method encapsulate all the\n",
      " |      logic for converting question(s) and context(s) to [`SquadExample`].\n",
      " |      \n",
      " |      We currently support extractive question answering.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          question (`str` or `List[str]`): The question(s) asked.\n",
      " |          context (`str` or `List[str]`): The context(s) in which we will look for the answer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          One or a list of [`SquadExample`]: The corresponding [`SquadExample`] grouping question and context.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  default_input_names = 'question,context'\n",
      " |  \n",
      " |  handle_impossible_answer = False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.ChunkPipeline:\n",
      " |  \n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (`List[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |      \n",
      " |      Return:\n",
      " |          `Dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |  \n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |  \n",
      " |  get_inference_context(self)\n",
      " |  \n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: str = None, commit_description: str = None, tags: Optional[List[str]] = None, **deprecated_kwargs) -> str from transformers.utils.hub.PushToHubMixin\n",
      " |      Upload the pipeline file to the ğŸ¤— Model Hub.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your pipe to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload pipe\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`List[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from transformers import pipeline\n",
      " |      \n",
      " |      pipe = pipeline(\"google-bert/bert-base-cased\")\n",
      " |      \n",
      " |      # Push the pipe to your namespace with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |      # Push the pipe to an organization with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |  \n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, **kwargs)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  torch_dtype\n",
      " |      Torch dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ko_qna)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
